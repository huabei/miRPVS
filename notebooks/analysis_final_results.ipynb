{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置工作目录\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"/mnt/f/SMTarRNA_total_results/total_dataset_results/total_dataset_results\")\n",
    "\n",
    "# hpc\n",
    "# os.chdir(\"/public/home/hpc192311018/Huabei/project/SMTARRNA-sync/SMTarRNA/data/total_dataset_results\")\n",
    "\n",
    "# 设置python工作目录\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_zinc_id(id: int):\n",
    "    \"\"\"create zinc id from int id\"\"\" \"\"\n",
    "    return \"ZINC\" + str(int(id + 1e12))[1:]\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/huabei/projects/SMTarRNA\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个文件夹下的结果\n",
    "import torch\n",
    "\n",
    "# folder = '3a6p'\n",
    "folder = \"4z4d\"\n",
    "file_lists = os.listdir(folder)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = torch.load(os.path.join(folder, file_lists[0]))\n",
    "data_: tuple\n",
    "data_[0][:5], data_[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len('ZINC000000001084')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将data_(n, 5)的第一列和data_(1, )的第一列拼接起来\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_data(folder, file_lists):\n",
    "    \"\"\"get data from folder and file_lists\"\"\"\n",
    "    data = []\n",
    "    for file in tqdm(file_lists):\n",
    "        data_ = torch.load(os.path.join(folder, file))\n",
    "        data_ = np.concatenate([data_[1].reshape(-1, 1), data_[0][:, 0].reshape(-1, 1)], axis=1)\n",
    "        data.append(data_)\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = get_data(folder, file_lists)\n",
    "# data_[0][:, 0].shape, data_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依据total_data第二列获取top千分之一的数据\n",
    "def get_top_data(total_data, top=0.001):\n",
    "    top_data = total_data[total_data[:, 1].argsort()][: int(total_data.shape[0] * top)]\n",
    "    return top_data\n",
    "\n",
    "\n",
    "top_data = get_top_data(total_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用脚本下载smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚合分别下载的smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"smiles\"\n",
    "files = os.listdir(folder)\n",
    "smiles_dict = dict()\n",
    "for file in tqdm(files):\n",
    "    # print(file)\n",
    "    with open(os.path.join(folder, file), \"r\", errors=\"ignore\") as f:\n",
    "        for line_ in f.readlines():\n",
    "            # print(line_)\n",
    "            # line_ = str(line_, encoding='utf-8')\n",
    "            # line_ = str(line_, encoding='utf-8')\n",
    "            try:\n",
    "                line = line_.strip().split()\n",
    "                smiles_dict[line[1]] = line[0]\n",
    "            # break\n",
    "            except Exception as e:\n",
    "                print(file)\n",
    "                # print(line_)\n",
    "                # print(line_.decode('utf-8'))\n",
    "                continue\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存为dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_id_smiles_frame = pd.DataFrame(smiles_dict.items(), columns=[\"zinc_id\", \"smiles\"])\n",
    "# 设置zinc_id为索引\n",
    "# zinc_id_smiles_frame.set_index('zinc_id', inplace=True)\n",
    "# 去除重复的zinc_id\n",
    "zinc_id_smiles_frame.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_id_smiles_frame.head()\n",
    "# 保存\n",
    "zinc_id_smiles_frame.to_csv(\"zinc_id_smiles_frame.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_id_smiles_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引出四个复合体分别的smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data_3a6p = torch.load(\"3a6p_top_data.pt\")\n",
    "data_4z4d = torch.load(\"4z4d_top_data.pt\")\n",
    "data_4z4c = torch.load(\"4z4c_top_data.pt\")\n",
    "data_6cbd = torch.load(\"6cbd_top_data.pt\")\n",
    "\n",
    "zinc_id_smiles_frame = pd.read_csv(\"zinc_id_smiles_frame.csv\")\n",
    "\n",
    "# 转换为dataframe\n",
    "data_3a6p = pd.DataFrame(data_3a6p, columns=[\"zinc_id\", \"score\"])\n",
    "data_4z4d = pd.DataFrame(data_4z4d, columns=[\"zinc_id\", \"score\"])\n",
    "data_4z4c = pd.DataFrame(data_4z4c, columns=[\"zinc_id\", \"score\"])\n",
    "data_6cbd = pd.DataFrame(data_6cbd, columns=[\"zinc_id\", \"score\"])\n",
    "\n",
    "\n",
    "# 转换zinc id列为int\n",
    "data_3a6p[\"zinc_id\"] = data_3a6p[\"zinc_id\"].apply(create_zinc_id)\n",
    "# data_3a6p.set_index('zinc_id', inplace=True)\n",
    "# 去除重复的zinc id\n",
    "# data_3a6p.drop_duplicates(inplace=True)\n",
    "data_4z4d[\"zinc_id\"] = data_4z4d[\"zinc_id\"].apply(create_zinc_id)\n",
    "# data_4z4d.set_index('zinc_id', inplace=True)\n",
    "# data_4z4d.drop_duplicates(inplace=True)\n",
    "data_4z4c[\"zinc_id\"] = data_4z4c[\"zinc_id\"].apply(create_zinc_id)\n",
    "# data_4z4c.set_index('zinc_id', inplace=True)\n",
    "# data_4z4c.drop_duplicates(inplace=True)\n",
    "data_6cbd[\"zinc_id\"] = data_6cbd[\"zinc_id\"].apply(create_zinc_id)\n",
    "# data_6cbd.set_index('zinc_id', inplace=True)\n",
    "# data_6cbd.drop_duplicates(inplace=True)\n",
    "\n",
    "# 合并表格\n",
    "data_3a6p_zinc_id_smiles_frame = pd.merge(\n",
    "    data_3a6p, zinc_id_smiles_frame, on=\"zinc_id\", how=\"inner\"\n",
    ")\n",
    "data_3a6p_zinc_id_smiles_frame.drop_duplicates(subset=[\"zinc_id\"], inplace=True)\n",
    "# 保存数据\n",
    "data_3a6p_zinc_id_smiles_frame.to_csv(\"data_3a6p_zinc_id_smiles_frame.csv\", index=False)\n",
    "\n",
    "data_4z4d_zinc_id_smiles_frame = pd.merge(\n",
    "    data_4z4d, zinc_id_smiles_frame, on=\"zinc_id\", how=\"inner\"\n",
    ")\n",
    "data_4z4d_zinc_id_smiles_frame.drop_duplicates(subset=[\"zinc_id\"], inplace=True)\n",
    "data_4z4d_zinc_id_smiles_frame.to_csv(\"data_4z4d_zinc_id_smiles_frame.csv\", index=False)\n",
    "\n",
    "data_4z4c_zinc_id_smiles_frame = pd.merge(\n",
    "    data_4z4c, zinc_id_smiles_frame, on=\"zinc_id\", how=\"inner\"\n",
    ")\n",
    "data_4z4c_zinc_id_smiles_frame.drop_duplicates(subset=[\"zinc_id\"], inplace=True)\n",
    "data_4z4c_zinc_id_smiles_frame.to_csv(\"data_4z4c_zinc_id_smiles_frame.csv\", index=False)\n",
    "\n",
    "data_6cbd_zinc_id_smiles_frame = pd.merge(\n",
    "    data_6cbd, zinc_id_smiles_frame, on=\"zinc_id\", how=\"inner\"\n",
    ")\n",
    "data_6cbd_zinc_id_smiles_frame.drop_duplicates(subset=[\"zinc_id\"], inplace=True)\n",
    "data_6cbd_zinc_id_smiles_frame.to_csv(\"data_6cbd_zinc_id_smiles_frame.csv\", index=False)\n",
    "\n",
    "data_3a6p.shape, data_4z4d.shape, data_4z4c.shape, data_6cbd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3a6p_zinc_id_smiles_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.ML.Cluster import Butina\n",
    "\n",
    "\n",
    "def ClusterFps(fps, cutoff=0.2):\n",
    "    # first generate the distance matrix:\n",
    "    dists = []\n",
    "    nfps = len(fps)\n",
    "    print(\"calculating distance matrix\")\n",
    "    for i in range(1, nfps):\n",
    "        sims = DataStructs.BulkTanimotoSimilarity(fps[i], fps[:i])\n",
    "        dists.extend([1 - x for x in sims])\n",
    "    print(\"cluster\")\n",
    "    # now cluster the data:\n",
    "    cs = Butina.ClusterData(dists, nfps, cutoff, isDistData=True)\n",
    "    return cs\n",
    "\n",
    "\n",
    "def calFP(smi):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, 1024)\n",
    "        return fp\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "data_3a6p_zinc_id_smiles_frame = pd.read_csv(\"data_3a6p_zinc_id_smiles_frame.csv\")\n",
    "data_4z4d_zinc_id_smiles_frame = pd.read_csv(\"data_4z4d_zinc_id_smiles_frame.csv\")\n",
    "data_4z4c_zinc_id_smiles_frame = pd.read_csv(\"data_4z4c_zinc_id_smiles_frame.csv\")\n",
    "data_6cbd_zinc_id_smiles_frame = pd.read_csv(\"data_6cbd_zinc_id_smiles_frame.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算fps\n",
    "data_3a6p_zinc_id_smiles_frame[\"fp\"] = data_3a6p_zinc_id_smiles_frame[\"smiles\"].apply(calFP)\n",
    "print(\"3a6p\")\n",
    "data_4z4d_zinc_id_smiles_frame[\"fp\"] = data_4z4d_zinc_id_smiles_frame[\"smiles\"].apply(calFP)\n",
    "print(\"4z4d\")\n",
    "data_4z4c_zinc_id_smiles_frame[\"fp\"] = data_4z4c_zinc_id_smiles_frame[\"smiles\"].apply(calFP)\n",
    "print(\"4z4c\")\n",
    "data_6cbd_zinc_id_smiles_frame[\"fp\"] = data_6cbd_zinc_id_smiles_frame[\"smiles\"].apply(calFP)\n",
    "print(\"6cbd\")\n",
    "# 去除None\n",
    "data_3a6p_zinc_id_smiles_frame = data_3a6p_zinc_id_smiles_frame[\n",
    "    data_3a6p_zinc_id_smiles_frame[\"fp\"].notnull()\n",
    "]\n",
    "data_4z4d_zinc_id_smiles_frame = data_4z4d_zinc_id_smiles_frame[\n",
    "    data_4z4d_zinc_id_smiles_frame[\"fp\"].notnull()\n",
    "]\n",
    "data_4z4c_zinc_id_smiles_frame = data_4z4c_zinc_id_smiles_frame[\n",
    "    data_4z4c_zinc_id_smiles_frame[\"fp\"].notnull()\n",
    "]\n",
    "data_6cbd_zinc_id_smiles_frame = data_6cbd_zinc_id_smiles_frame[\n",
    "    data_6cbd_zinc_id_smiles_frame[\"fp\"].notnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为pt\n",
    "data_3a6p_zinc_id_smiles_frame.to_pickle(\"data_3a6p_zinc_id_smiles_fp_frame4.pkl\", protocol=3)\n",
    "data_4z4d_zinc_id_smiles_frame.to_pickle(\"data_4z4d_zinc_id_smiles_fp_frame4.pkl\", protocol=3)\n",
    "data_4z4c_zinc_id_smiles_frame.to_pickle(\"data_4z4c_zinc_id_smiles_fp_frame4.pkl\", protocol=3)\n",
    "data_6cbd_zinc_id_smiles_frame.to_pickle(\"data_6cbd_zinc_id_smiles_fp_frame4.pkl\", protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_fp_dict = {\n",
    "    \"3a6p\": data_3a6p_zinc_id_smiles_frame,\n",
    "    \"4z4d\": data_4z4d_zinc_id_smiles_frame,\n",
    "    \"4z4c\": data_4z4c_zinc_id_smiles_frame,\n",
    "    \"6cbd\": data_6cbd_zinc_id_smiles_frame,\n",
    "}\n",
    "\n",
    "# 使用pickle保存\n",
    "import pickle\n",
    "\n",
    "with open(\"data_total_zinc_id_smiles_fp_frame_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(complex_fp_dict, f, protocol=3)\n",
    "\n",
    "# 读取数据\n",
    "with open(\"fps/data_total_zinc_id_smiles_fp_frame_dict.pkl\", \"rb\") as f:\n",
    "    complex_fp_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "data_3a6p_zinc_id_smiles_frame = pd.read_pickle(\"fps/data_3a6p_zinc_id_smiles_fp_frame.pkl\")\n",
    "data_4z4d_zinc_id_smiles_frame = pd.read_pickle(\"fps/data_4z4d_zinc_id_smiles_fp_frame.pkl\")\n",
    "data_4z4c_zinc_id_smiles_frame = pd.read_pickle(\"fps/data_4z4c_zinc_id_smiles_fp_frame.pkl\")\n",
    "data_6cbd_zinc_id_smiles_frame = pd.read_pickle(\"fps/data_6cbd_zinc_id_smiles_fp_frame.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_dict = dict()\n",
    "zinc_id_dict = dict()\n",
    "scores_dict = dict()\n",
    "\n",
    "smiles_dict[\"3a6p\"] = data_3a6p_zinc_id_smiles_frame[\"smiles\"].to_list()\n",
    "smiles_dict[\"4z4d\"] = data_4z4d_zinc_id_smiles_frame[\"smiles\"].to_list()\n",
    "smiles_dict[\"4z4c\"] = data_4z4c_zinc_id_smiles_frame[\"smiles\"].to_list()\n",
    "smiles_dict[\"6cbd\"] = data_6cbd_zinc_id_smiles_frame[\"smiles\"].to_list()\n",
    "\n",
    "zinc_id_dict[\"3a6p\"] = data_3a6p_zinc_id_smiles_frame[\"zinc_id\"].to_list()\n",
    "zinc_id_dict[\"4z4d\"] = data_4z4d_zinc_id_smiles_frame[\"zinc_id\"].to_list()\n",
    "zinc_id_dict[\"4z4c\"] = data_4z4c_zinc_id_smiles_frame[\"zinc_id\"].to_list()\n",
    "zinc_id_dict[\"6cbd\"] = data_6cbd_zinc_id_smiles_frame[\"zinc_id\"].to_list()\n",
    "\n",
    "scores_dict[\"3a6p\"] = data_3a6p_zinc_id_smiles_frame[\"score\"].to_list()\n",
    "scores_dict[\"4z4d\"] = data_4z4d_zinc_id_smiles_frame[\"score\"].to_list()\n",
    "scores_dict[\"4z4c\"] = data_4z4c_zinc_id_smiles_frame[\"score\"].to_list()\n",
    "scores_dict[\"6cbd\"] = data_6cbd_zinc_id_smiles_frame[\"score\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3a6p_zinc_id_smiles_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将各个复合物的smiles写入txt文件，加入index, 用于chemfp输入\n",
    "\n",
    "for key, value in complex_fp_dict.items():\n",
    "    with open(key + \"_smiles.smi\", \"w\") as f:\n",
    "        for i, smiles in enumerate(value[\"smiles\"]):\n",
    "            f.write(smiles + \"\\t\" + str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理chemfp聚类结果\n",
    "\n",
    "\n",
    "def read_chemfp_cluster(file: str):\n",
    "    \"\"\"\n",
    "    用以分析使用chemfp脚本聚类之后的输出文件\n",
    "    :param file: chemfp聚类输出文件\n",
    "    :return: 聚类中心，聚类成员\n",
    "    \"\"\"\n",
    "    f = open(file, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    # 筛选以数字开头的行\n",
    "    lines_central = [int(line.split(\" \")[0]) for line in lines[7:] if line.split(\" \")[0].isdigit()]\n",
    "    # 筛选以=>开头的行\n",
    "    lines_members = [\n",
    "        [int(s) for s in line.split(\" \")[7:]]\n",
    "        for line in lines[6:]\n",
    "        if line.split(\" \")[0].startswith(\"=>\")\n",
    "    ]\n",
    "    assert len(lines_central) == len(lines_members)\n",
    "\n",
    "    return zip(lines_central, lines_members)\n",
    "\n",
    "\n",
    "# 读取聚类结果\n",
    "import random\n",
    "\n",
    "complex_name = [\"3a6p\", \"4z4d\", \"4z4c\", \"6cbd\"]\n",
    "complex_clusters = dict()\n",
    "for cmpx in complex_name:\n",
    "    complex_clusters[cmpx] = read_chemfp_cluster(f\"chemfp_clustering/{cmpx}-mol-0.6.clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类结果采样\n",
    "def sample_clusters_rep(complex_clusters: dict, top=10):\n",
    "    clusters_rep = dict()\n",
    "    for cmpx, clusters in complex_clusters.items():\n",
    "        clusters_representation = []\n",
    "        for cluster in list(clusters)[:top]:\n",
    "            clusters_representation.append([cluster[0]] + random.sample(cluster[1], 5))\n",
    "        clusters_rep[cmpx] = clusters_representation\n",
    "    return clusters_rep\n",
    "\n",
    "\n",
    "clusters_rep = sample_clusters_rep(complex_clusters)\n",
    "clusters_rep[\"3a6p\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存聚类代表分子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将代表分保存为txt文件\n",
    "with open(\"clusters_rep.txt\", \"w\") as f:\n",
    "    for cmpx, clusters in clusters_rep.items():\n",
    "        f.write(cmpx + \"\\n\")\n",
    "        for cluster in clusters:\n",
    "            f.write(\" \".join([zinc_id_dict[cmpx][c] for c in cluster]) + \"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将聚类结果处理为rdkit mol文件\n",
    "from collections import namedtuple\n",
    "\n",
    "Mol = namedtuple(\"Mol\", [\"mol\", \"smiles\", \"zinc_id\", \"score\"])\n",
    "\n",
    "\n",
    "def get_mol(cluster_index, cmpx):\n",
    "    \"\"\"get mol from cluster_index and cmpx\"\"\"\n",
    "    return Mol(\n",
    "        mol=Chem.MolFromSmiles(smiles_dict[cmpx][cluster_index]),\n",
    "        smiles=smiles_dict[cmpx][cluster_index],\n",
    "        zinc_id=zinc_id_dict[cmpx][cluster_index],\n",
    "        score=scores_dict[cmpx][cluster_index],\n",
    "    )\n",
    "\n",
    "\n",
    "clusters_rep_mol = dict()\n",
    "for cmpx, clusters in clusters_rep.items():\n",
    "    clusters_rep_mol[cmpx] = []\n",
    "    for cluster in clusters:\n",
    "        mols = [get_mol(c, cmpx) for c in cluster]\n",
    "        clusters_rep_mol[cmpx].append(mols)\n",
    "\n",
    "\n",
    "# ms = []\n",
    "# label = []\n",
    "# scores = []\n",
    "\n",
    "# for c in clusters_representation:\n",
    "#     for i in c:\n",
    "#         smi = smiles_dict['3a6p'][i]\n",
    "#         label.append(zinc_id_dict['3a6p'][i])\n",
    "#         scores.append(str(scores_dict['3a6p'][i]))\n",
    "#         ms.append(Chem.MolFromSmiles(smi))\n",
    "#         # print(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "IPythonConsole.drawOptions.addAtomIndices = False\n",
    "\n",
    "for k, v in clusters_rep_mol.items():\n",
    "    print(k)\n",
    "    t = 0\n",
    "    for i in v:\n",
    "        ms = [m.mol for m in i]\n",
    "        scores = [str(m.zinc_id) for m in i]\n",
    "        image = Draw.MolsToGridImage(\n",
    "            ms,\n",
    "            molsPerRow=6,\n",
    "            subImgSize=(200, 200),\n",
    "            legends=scores,\n",
    "            maxMols=60,\n",
    "            useSVG=True,\n",
    "            returnPNG=False,\n",
    "        )\n",
    "        # 保存图片\n",
    "        with open(f\"images/{k}_cluster_{t}.svg\", \"w\") as f:\n",
    "            f.write(image.data)\n",
    "        t += 1\n",
    "    #     break\n",
    "    # break\n",
    "# 测试画图\n",
    "# ms = [m.mol for m in clusters_rep_mol['3a6p'][0]]\n",
    "# scores = [str(m.zinc_id) for m in clusters_rep_mol['3a6p'][0]]\n",
    "# draw = Draw.MolsToGridImage(ms, molsPerRow=6, subImgSize=(200, 200), legends=scores,useSVG=True,maxMols=60)\n",
    "# 保存图片\n",
    "# draw.save('3a6p_cluster_top_10.svg')\n",
    "\n",
    "# IPythonConsole.SVG(draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类\n",
    "data_3a6p_zinc_id_smiles_frame[\"cluster\"] = ClusterFps(\n",
    "    data_3a6p_zinc_id_smiles_frame[\"fp\"].tolist(), cutoff=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from smilesgenerate fingerprints\n",
    "f = open(\"total_smiles_400000_410000.smi\", \"r\")\n",
    "ms = []\n",
    "try:\n",
    "    for x in f:\n",
    "        ms.append(Chem.MolFromSmiles(x.split()[0]))\n",
    "except Exception as e:\n",
    "    print(x.split()[0])\n",
    "# ms = [Chem.MolFromSmiles(x.split()[0]) for x in f]\n",
    "f.close()\n",
    "# ms = [x for x in Chem.CanonSmiles() if x is not None]\n",
    "fps = [AllChem.GetMorganFingerprintAsBitVect(x, 2, 1024) for x in ms]\n",
    "\n",
    "# cluster\n",
    "clusters = ClusterFps(fps, cutoff=0.8)\n",
    "\n",
    "# show one of the clusters\n",
    "# print(clusters[20])\n",
    "\n",
    "# now display structures from one of the clusters\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "# look at a specific cluster\n",
    "# m1 = ms[1630]\n",
    "# m2 = ms[1010]\n",
    "# m3 = ms[1022]\n",
    "# m4 = ms[1023]\n",
    "# m5 = ms[1034]\n",
    "# m6 = ms[1043]\n",
    "# mols=(m1,m2,m3,m4,m5,m6)\n",
    "# Draw.MolsToGridImage(mols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对接验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"/mnt/f/SMTarRNA_total_results/total_dataset_results/total_dataset_results/top_0_001\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dock_score(file_name):\n",
    "    \"\"\"read dock score from file_name\"\"\"\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_total_energy(data):\n",
    "    \"\"\"get total energy from data\"\"\"\n",
    "    return [v[0, 0] for v in data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_3a6p_1 = get_total_energy(read_dock_score('val/3a6p_top_data_10k_3a6p_dock_energy_None-None_32_20230916174351.pkl'))\n",
    "# data_3a6p_2 = get_total_energy(read_dock_score('val/3a6p_top_data_10k_3a6p_dock_energy_None-None_32_20230918215122.pkl'))\n",
    "\n",
    "# pearson_ = np.corrcoef(data_3a6p_1, data_3a6p_2)[0, 1]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# ax.scatter(data_3a6p_1, data_3a6p_2, s=1)\n",
    "# pearson_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = os.listdir(\"val\")\n",
    "pkl_files = [file for file in pkl_files if file.endswith(\".pkl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_name = [\"3a6p\", \"4z4d\", \"4z4c\", \"6cbd\"]\n",
    "results = dict()\n",
    "for file in pkl_files:\n",
    "    c = file[:4]\n",
    "    results[c] = read_dock_score(os.path.join(\"val\", file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_dict(file: str):\n",
    "    \"\"\"read csv file to dict\"\"\"\n",
    "    data = np.loadtxt(file, delimiter=\",\")\n",
    "    res = dict()\n",
    "    for d in data:\n",
    "        res[create_zinc_id(int(d[0]))] = d[1]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_e = {k: read_csv_to_dict(f\"{k}_top_data.csv\") for k in complex_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = dict()\n",
    "for c in complex_name:\n",
    "    # 实验结果\n",
    "    e = []\n",
    "    p = []\n",
    "    for k, v in results[c].items():\n",
    "        if v[0, 0] > -2:\n",
    "            print(k, v[0, 0])\n",
    "            continue\n",
    "        e.append(v[0, 0])\n",
    "        p.append(pre_e[c][k])\n",
    "    final_results[c + \"_e\"] = e\n",
    "    final_results[c + \"_p\"] = p\n",
    "    pearson = np.corrcoef(e, p)[0, 1]\n",
    "    print(f\"max dock score: {max(e)}\")\n",
    "    print(f\"max pre dock score: {max(p)}\")\n",
    "    print(f\"{c} pearson: {pearson}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in final_results[\"4z4d_e\"]:\n",
    "    if i > -2:\n",
    "        print(i)\n",
    "max(final_results[\"4z4d_e\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 绘制分布对比图\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 7))\n",
    "for i, c in enumerate(complex_name):\n",
    "    index = (i // 2, i % 2)\n",
    "    sns.histplot(final_results[c + \"_e\"], ax=ax[index], label=\"dock score\")\n",
    "    sns.histplot(final_results[c + \"_p\"], ax=ax[index], label=\"predict score\")\n",
    "    ax[index].legend()\n",
    "    # plt.savefig(f'{c}_distplot.svg')\n",
    "plt.show()\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in complex_name:\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(final_results[c + \"_e\"], final_results[c + \"_p\"], \"o\", label=\"dock score\")\n",
    "    ax.plot([-12, -8], [-12, -8])\n",
    "    # plt.savefig(f'{c}_scatter.svg')\n",
    "    plt.show()\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
