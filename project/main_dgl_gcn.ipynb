{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from model.dgl_gcn import LigandDataset_dgl, collate_fn, MolecularGCN\n",
    "from utils.package import plot_fit_confidence_bond\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from collections import defaultdict\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = '3dgnn'\n",
    "wandb.login(key='local-8fe6e6b5840c4c05aaaf6aac5ca8c1fb58abbd1f', host='http://localhost:8080')\n",
    "wandb.init(project=project, save_code=True, dir='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuabei\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e/Python_Project/SMTarRNA/project/wandb/run-20220620_170044-3stce0an</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/huabei/SMTarRNA-project/runs/3stce0an\" target=\"_blank\">cosmic-pond-2</a></strong> to <a href=\"http://localhost:8080/huabei/SMTarRNA-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/huabei/anaconda3/envs/smtr/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /mnt/e/Python_Project/SMTarRNA/project/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | embed_atom | Embedding  | 2.0 K \n",
      "1 | gamma      | ModuleList | 24    \n",
      "2 | conv       | ModuleList | 795 K \n",
      "3 | dense      | ModuleList | 526 K \n",
      "4 | dense_out  | Linear     | 257   \n",
      "------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.297     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a5b03ef0654659a3b3fd03e2ab91c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huabei/anaconda3/envs/smtr/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/huabei/anaconda3/envs/smtr/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509281ce3fab4041a4ca9a0ec77e4aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ba0841ec05418a9c405f4c7237ddee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huabei/anaconda3/envs/smtr/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test() got an unexpected keyword argument 'dataloders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/huabei/Projects/SMTarRNA/project/main.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=64'>65</a>\u001b[0m parser \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer\u001b[39m.\u001b[39madd_argparse_args(parser)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=65'>66</a>\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args([\u001b[39m'\u001b[39m\u001b[39m--dataset_name\u001b[39m\u001b[39m'\u001b[39m, dataset_name, \u001b[39m'\u001b[39m\u001b[39m--raw_dir\u001b[39m\u001b[39m'\u001b[39m, raw_dir,\u001b[39m'\u001b[39m\u001b[39m--save_dir\u001b[39m\u001b[39m'\u001b[39m, save_dir, \u001b[39m'\u001b[39m\u001b[39m--learning_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m0.0001\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=66'>67</a>\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39m--gpus=1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m--max_epochs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m1000\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=68'>69</a>\u001b[0m main(args)\n",
      "\u001b[1;32m/home/huabei/Projects/SMTarRNA/project/main.ipynb Cell 3'\u001b[0m in \u001b[0;36mmain\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=27'>28</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=28'>29</a>\u001b[0m \u001b[39m# trainer.fit(model, train_dataloader, val_dataloader)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=29'>30</a>\u001b[0m \u001b[39m# trainer.save_checkpoint(time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) + \".ckpt\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=30'>31</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(model, dataloders\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mval_dataloader(), verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(model\u001b[39m.\u001b[39mpredictions[\u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTarRNA/project/main.ipynb#ch0000001vscode-remote?line=32'>33</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(model\u001b[39m.\u001b[39mpredictions[\u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: test() got an unexpected keyword argument 'dataloders'"
     ]
    }
   ],
   "source": [
    "# %%wandb\n",
    "def main(hparams):\n",
    "\n",
    "    model_name = f'3dgnn-dim-{hparams.dim}-' + time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "    dict_args = vars(hparams)\n",
    "    model = MolecularGCN(**dict_args)\n",
    "    # logger\n",
    "    wandb_logger = pl.loggers.WandbLogger()\n",
    "    # callbacks\n",
    "    # early stopping\n",
    "    early_stopping = pl.callbacks.early_stopping.EarlyStopping(monitor='val_loss', patience=20, mode='min')\n",
    "    # checkpoint\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss', mode='min', save_last=True,\n",
    "                                                         dirpath='checkpoints', filename=model_name)\n",
    "    if hparams.checkpoint == None:\n",
    "        trainer = Trainer.from_argparse_args(hparams, logger=wandb_logger, auto_lr_find=True, callbacks=[early_stopping, checkpoint_callback])\n",
    "    else:\n",
    "        trainer = Trainer(resume_from_checkpoint=hparams.checkpoint, callbacks=[early_stopping])\n",
    "\n",
    "    trainer.fit(model)\n",
    "    # trainer.save_checkpoint(time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) + \".ckpt\")\n",
    "    trainer.test(model, dataloaders=model.val_dataloader(), verbose=False)\n",
    "    x = np.array(model.predictions['true'])\n",
    "    y = np.array(model.predictions['pred'])\n",
    "    val_r2 = r2_score(x, y)\n",
    "    val_fig = plot_fit_confidence_bond(x, y, val_r2, annot=False)\n",
    "    \n",
    "    model.predictions = defaultdict(list)\n",
    "    trainer.test(model, dataloaders=model.train_dataloader(), verbose=False)\n",
    "    x = np.array(model.predictions['true'])\n",
    "    y = np.array(model.predictions['pred'])\n",
    "    train_r2 = r2_score(x, y)\n",
    "    train_fig = plot_fit_confidence_bond(x, y, train_r2, annot=False)\n",
    "    if False:\n",
    "        wandb.log({'train_res': train_fig, 'val_res': val_fig})\n",
    "        wandb.log({'val_r2': val_r2, 'train_r2':train_r2})\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--dim\", type=int, default=256)\n",
    "    parser.add_argument(\"--batch_size\",type=int, default=256)\n",
    "    parser.add_argument(\"--raw_dir\",type=str, default=None)\n",
    "    parser.add_argument(\"--save_dir\",type=str, default=None)\n",
    "    parser.add_argument(\"--dataset_name\",type=str, default=None)\n",
    "    parser.add_argument(\"--checkpoint\",type=str, default=None)\n",
    "    # parser.add_argument(\"--max_epoch\",type=int, default=500)\n",
    "    raw_dir = 'data'\n",
    "    save_dir = 'data'\n",
    "    # add model args\n",
    "    dataset_name = 'in_man_exhaustiveness_96_dock_conformation'\n",
    "    # dataset_name = 'data_test'\n",
    "    parser = MolecularGCN.add_model_specific_args(parent_parser=parser)\n",
    "    # add Trainer args\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args(['--dataset_name', dataset_name, '--raw_dir', raw_dir,'--save_dir', save_dir, '--learning_rate', '0.0001',\n",
    "                                 '--gpus=1', '--max_epochs', '1000'])\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__\n"
     ]
    }
   ],
   "source": [
    "print(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('smtr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a75462293d05fc3e00128f4985dd13fcf50f4f5144b1474848efbcac1f09cd24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
