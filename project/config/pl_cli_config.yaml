# lightning.pytorch==2.0.1.post0
# fit:

seed_everything: 1234
# project: egnn_large
project: test
comment: multilable_little_large
group: ~
wandb: true
tune: false
trainer:
  devices: auto
  num_nodes: 1
  precision: 32-true
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        name: tensorboard
        save_dir: '' # will be overwrittern by trainer.default_root_dir
        default_hp_metric: false
    - class_path: lightning.pytorch.loggers.WandbLogger
      init_args:
        project: '' # will be overwrittern by project
        log_model: true
        dir: '' # will be overwrittern by trainer.default_root_dir
        notes: '' # will be overwrittern by comment
        offline: false
        group: '' # will be overwrittern by group
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val_loss
        mode: min
        save_top_k: 1
        save_last: true
        filename: '{epoch}-{val_loss:.4f}' # will be overwrittern by project
        dirpath: 'checkpoint'
  fast_dev_run: false
  max_epochs: 1
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  accumulate_grad_batches: 1
data:
  num_workers: 8
  dataset: 'zinc_complex3a6p_data_test'
  batch_size: 64
  data_dir: 'data/dataset/dataset/3a6p_100w'
ckpt_path: null
model:
  class_path: models.EgnnMultilable
  init_args:
    in_node_nf: 11
    hidden_nf: 128
    out_node_nf: 5
    batch_size: 64
    loss: smooth_l1
    lr: 0.0005
    lr_decay_min_lr: 0
    weight_decay: 0
    lr_scheduler: cosine
    lr_t_0: 2
    lr_t_mul: 2
    lr_t_max: 10 # cosine
    in_edge_nf: 0
    act_fn: silu
    n_layers: 1
    residual: true
    attention: true
    normalize: false
    tanh: false

