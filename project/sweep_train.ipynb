{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from main import main\n",
    "import ml_collections as mlc\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from model import MInterface\n",
    "from data import DInterface\n",
    "from train_utils import load_logger, load_callbacks\n",
    "from config.config import set_default_config\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "DEFAULT_CONFIG = mlc.ConfigDict()\n",
    "set_default_config(DEFAULT_CONFIG)\n",
    "with open('config/egnn_cfg_sweep.yaml', 'r') as f:\n",
    "    default_arg = yaml.safe_load(f)\n",
    "DEFAULT_CONFIG.update(default_arg)\n",
    "DEFAULT_CONFIG.log_dir = f'./log/sweep/{DEFAULT_CONFIG.pl_module.model_name}/{DEFAULT_CONFIG.pl_data_module.dataset}'\n",
    "if not os.path.exists(DEFAULT_CONFIG.log_dir):\n",
    "    os.makedirs(DEFAULT_CONFIG.log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sweep_main():\n",
    "    logging.info('seed everything')\n",
    "    pl.seed_everything(DEFAULT_CONFIG.seed)\n",
    "    if True:\n",
    "        logging.info('Using wandb')\n",
    "        wandb.init(project=DEFAULT_CONFIG.project, group=DEFAULT_CONFIG.group, save_code=True, dir=DEFAULT_CONFIG.log_dir, reinit=True)\n",
    "        args = DEFAULT_CONFIG\n",
    "        args.update_from_flattened_dict(vars(wandb.config)['_items'])\n",
    "        # raise Exception\n",
    "        wandb.config.update(args.pl_module.model.to_dict())\n",
    "        wandb.config.update(args.to_dict())\n",
    "    logging.info('Loading data and model')\n",
    "    data_module = DInterface(**args.pl_data_module)\n",
    "    model = MInterface(**args.pl_module)\n",
    "\n",
    "    logging.info('loading callbacks and logger')\n",
    "    args.trainer.callbacks = load_callbacks(args)\n",
    "    args.trainer.logger = load_logger(args)\n",
    "    \n",
    "    logging.info('creating trainer')\n",
    "    trainer = Trainer(**args.trainer)\n",
    "    \n",
    "    if args.trainer.auto_scale_batch_size:\n",
    "        logging.info('start auto scale batch size')\n",
    "        trainer.tune(model, data_module)\n",
    "\n",
    "    logging.info('start training')\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    logging.info('start testing')\n",
    "    trainer.test(model, data_module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 22:07:20,610 - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuabei\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for localhost to your netrc file: /home/huabei/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='local-8fe6e6b5840c4c05aaaf6aac5ca8c1fb58abbd1f', host='http://localhost:8080')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: nw1xnlx2\n",
      "Sweep URL: http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://docs.wandb.ai/guides/sweeps/define-sweep-configuration\n",
    "sweep_config = {\n",
    "  \"name\" : \"my-sweep\",\n",
    "  \"method\" : \"random\",\n",
    "  \"metric\" : {'goal': 'minimize', 'name': 'val_loss'},\n",
    "  \"parameters\" : {\n",
    "    \"pl_module.lr\": {\n",
    "      \"distribution\": \"log_uniform_values\",\n",
    "      \"min\": 0.00005,\n",
    "      \"max\": 0.005\n",
    "    },\n",
    "    # \"pl_module.model.tanh\": {\n",
    "    #   \"values\": [True, False]\n",
    "    # }\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=DEFAULT_CONFIG.project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 22:07:28,662 - INFO - Starting sweep agent: entity=None, project=None, count=15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cswkkyb7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0010574768573226232\n",
      "2023-03-15 22:07:28,844 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 22:07:28,848 - INFO - Using wandb\n",
      "2023-03-15 22:07:28,854 - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_220729-cswkkyb7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/cswkkyb7' target=\"_blank\">colorful-sweep-1</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/cswkkyb7' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/cswkkyb7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 22:07:38,454 - INFO - Loading data and model\n",
      "2023-03-15 22:07:40,586 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "2023-03-15 22:07:40,618 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 22:07:40,766 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 22:33:54,359 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.21898266673088074\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2aa7621bbd42c18529e8ff0e0753ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.120 MB of 8.120 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▄▅▅▃▃▃▃▃▄▄▂▂▄▂▂▃▃▁▃▂▃▂▄▂▃▄▂▂▂▃▁▃▂▁▂▂▂▁</td></tr><tr><td>train_mae</td><td>█▅▄▄▃▃▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▆▃▃▃▃▃▂▃▃▃▂▂▂▂▁▁▁▂▁▂▂▁▁▃▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▆▆▆▃▄▃▃▃▂▃▃▃▂▂▂▂▂▁▁▂▂▂▃▁▂▄▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.21898</td></tr><tr><td>test_r2</td><td>0.8272</td></tr><tr><td>train_loss</td><td>0.02419</td></tr><tr><td>train_mae</td><td>0.20011</td></tr><tr><td>train_r2</td><td>0.85673</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.0396</td></tr><tr><td>val_mae</td><td>0.2182</td></tr><tr><td>val_r2</td><td>0.82865</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-sweep-1</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/cswkkyb7' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/cswkkyb7</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_220729-cswkkyb7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z5hqm9se with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.00253837934061579\n",
      "2023-03-15 22:34:03,274 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 22:34:03,275 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_223404-z5hqm9se</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/z5hqm9se' target=\"_blank\">leafy-sweep-2</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/z5hqm9se' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/z5hqm9se</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 22:34:11,028 - INFO - Loading data and model\n",
      "2023-03-15 22:34:12,894 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 22:34:12,896 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 22:34:13,001 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 23:00:14,187 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22140133380889893\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57b110705a74c2296b8c65b0e0cf43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.129 MB of 8.129 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>██▇▅▅▄▃▃▄▃▄▄▃▃▅▂▂▃▃▁▆▄▄▂▅▄▂▄▃▄▃▃▂▄▂▁▂▂▂▂</td></tr><tr><td>train_mae</td><td>█▅▄▃▂▄▃▂▂▂▃▂▂▂▂▂▂▁▁▁▄▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▃▄▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▃▃▄▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>1e-05</td></tr><tr><td>test_mae</td><td>0.2214</td></tr><tr><td>test_r2</td><td>0.82388</td></tr><tr><td>train_loss</td><td>0.02644</td></tr><tr><td>train_mae</td><td>0.20214</td></tr><tr><td>train_r2</td><td>0.85371</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.03981</td></tr><tr><td>val_mae</td><td>0.21912</td></tr><tr><td>val_r2</td><td>0.82769</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-sweep-2</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/z5hqm9se' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/z5hqm9se</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_223404-z5hqm9se/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rhhvl23p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.001796409084297166\n",
      "2023-03-15 23:00:22,919 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 23:00:22,920 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_230023-rhhvl23p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/rhhvl23p' target=\"_blank\">swept-sweep-3</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/rhhvl23p' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/rhhvl23p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 23:00:30,751 - INFO - Loading data and model\n",
      "2023-03-15 23:00:32,605 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 23:00:32,606 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 23:00:32,726 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 23:26:34,672 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22484564781188965\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73415783a9b34d339753931e1b653727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.139 MB of 8.139 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>▅▄█▄▄▃▂▂▂▂▅▅▂▃▄▂▂▃▃▁▄▂▃▁▃▂▁▃▂▃▂▂▁▃▁▁▂▂▁▂</td></tr><tr><td>train_mae</td><td>█▅▄▃▂▃▃▂▂▂█▄▃▃▂▂▂▂▂▂▆▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▃▄▂▂▂▂▂▁▄▃▃▂▃▂▂▂▂▁▄▂▂▂▄▂▄▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▃▃▄▂▂▂▂▂▁▅▃▃▂▃▂▂▂▂▁▄▂▂▂▄▂▅▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.22485</td></tr><tr><td>test_r2</td><td>0.82029</td></tr><tr><td>train_loss</td><td>0.02775</td></tr><tr><td>train_mae</td><td>0.21388</td></tr><tr><td>train_r2</td><td>0.83621</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.04055</td></tr><tr><td>val_mae</td><td>0.22174</td></tr><tr><td>val_r2</td><td>0.82444</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-sweep-3</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/rhhvl23p' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/rhhvl23p</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_230023-rhhvl23p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oqsp8rpx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0007383774904333095\n",
      "2023-03-15 23:26:46,402 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 23:26:46,404 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_232647-oqsp8rpx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/oqsp8rpx' target=\"_blank\">amber-sweep-4</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/oqsp8rpx' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/oqsp8rpx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 23:26:54,244 - INFO - Loading data and model\n",
      "2023-03-15 23:26:56,012 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 23:26:56,014 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 23:26:56,124 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 23:53:09,724 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22081753611564636\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9948c053164c438979a9761aafd6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.148 MB of 8.148 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▄▅▅▃▃▃▃▃▄▄▂▂▄▃▂▃▃▁▃▂▃▁▃▃▃▃▂▂▂▃▁▃▂▁▂▂▂▂</td></tr><tr><td>train_mae</td><td>█▄▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▃▂▂▂▂▂▁▁▂▂▂▂▁▂▃▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▄▄▃▃▃▃▃▂▃▂▃▃▂▂▂▂▂▁▂▂▂▂▁▂▃▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.22082</td></tr><tr><td>test_r2</td><td>0.82627</td></tr><tr><td>train_loss</td><td>0.02553</td></tr><tr><td>train_mae</td><td>0.21038</td></tr><tr><td>train_r2</td><td>0.84167</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.03965</td></tr><tr><td>val_mae</td><td>0.21896</td></tr><tr><td>val_r2</td><td>0.82831</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-4</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/oqsp8rpx' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/oqsp8rpx</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_232647-oqsp8rpx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hqkfyrfr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.00025477442904410264\n",
      "2023-03-15 23:53:20,032 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 23:53:20,033 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_235320-hqkfyrfr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/hqkfyrfr' target=\"_blank\">frosty-sweep-5</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/hqkfyrfr' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/hqkfyrfr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 23:53:27,783 - INFO - Loading data and model\n",
      "2023-03-15 23:53:29,526 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 23:53:29,527 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 23:53:29,641 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 00:19:34,024 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.23115329444408417\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9551fe04119b4c288660d3a864e5b5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.159 MB of 8.159 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▄▅▆▄▃▄▄▃▃▅▃▃▅▃▂▄▃▂▄▄▄▂▄▃▂▄▂▃▂▄▁▄▂▂▂▃▂▂</td></tr><tr><td>train_mae</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▅▄▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.23115</td></tr><tr><td>test_r2</td><td>0.81017</td></tr><tr><td>train_loss</td><td>0.03375</td></tr><tr><td>train_mae</td><td>0.22949</td></tr><tr><td>train_r2</td><td>0.81281</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.04317</td></tr><tr><td>val_mae</td><td>0.22918</td></tr><tr><td>val_r2</td><td>0.81293</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-sweep-5</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/hqkfyrfr' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/hqkfyrfr</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_235320-hqkfyrfr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: an1fddhi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0023095000732364667\n",
      "2023-03-16 00:19:43,665 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 00:19:43,666 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_001944-an1fddhi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/an1fddhi' target=\"_blank\">dainty-sweep-6</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/an1fddhi' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/an1fddhi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 00:19:51,428 - INFO - Loading data and model\n",
      "2023-03-16 00:19:53,249 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 00:19:53,252 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 00:19:53,369 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 00:45:58,717 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22668665647506714\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▇▄▅▅▅▃▄▃▃▄▃▃▃▅▃▃▃▄▂▄▄▄▂▄▃▃▄▃▃▃▃▂▄▂▁▁▂▁▂</td></tr><tr><td>train_mae</td><td>█▅▄▄▃▅▃▃▃▂▃▃▃▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▄▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▄▃▂▂▂▂▂▁▂▂▃▂▁▁▁▁▁▁▁▁▁▁▂▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>1e-05</td></tr><tr><td>test_mae</td><td>0.22669</td></tr><tr><td>test_r2</td><td>0.81569</td></tr><tr><td>train_loss</td><td>0.02104</td></tr><tr><td>train_mae</td><td>0.18871</td></tr><tr><td>train_r2</td><td>0.87259</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.0418</td></tr><tr><td>val_mae</td><td>0.22461</td></tr><tr><td>val_r2</td><td>0.81909</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-sweep-6</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/an1fddhi' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/an1fddhi</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_001944-an1fddhi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6tz2y3ko with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 8.596591528094639e-05\n",
      "2023-03-16 00:46:09,172 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 00:46:09,172 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_004610-6tz2y3ko</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/6tz2y3ko' target=\"_blank\">sweepy-sweep-7</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/6tz2y3ko' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/6tz2y3ko</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 00:46:17,061 - INFO - Loading data and model\n",
      "2023-03-16 00:46:18,869 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 00:46:18,874 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 00:46:19,000 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 01:12:24,357 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.2535140812397003\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8766d73e9dfc43c4a7d9048fb989f81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.181 MB of 8.181 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▇▆▆▄▃▅▄▄▄▃▃▄▄▄▂▄▃▂▄▄▃▃▄▃▂▄▃▃▃▃▁▄▂▃▃▃▂▃</td></tr><tr><td>train_mae</td><td>█▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▅▅▄▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▇▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▂▁▂▁▁▂▁▁▁▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.25351</td></tr><tr><td>test_r2</td><td>0.77289</td></tr><tr><td>train_loss</td><td>0.04082</td></tr><tr><td>train_mae</td><td>0.25362</td></tr><tr><td>train_r2</td><td>0.77166</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.0513</td></tr><tr><td>val_mae</td><td>0.25085</td></tr><tr><td>val_r2</td><td>0.77725</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweepy-sweep-7</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/6tz2y3ko' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/6tz2y3ko</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_004610-6tz2y3ko/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d195z0pb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0008933179783669317\n",
      "2023-03-16 01:12:32,817 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 01:12:32,818 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_011233-d195z0pb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/d195z0pb' target=\"_blank\">lemon-sweep-8</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/d195z0pb' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/d195z0pb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 01:12:40,659 - INFO - Loading data and model\n",
      "2023-03-16 01:12:42,545 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 01:12:42,547 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 01:12:42,656 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 01:38:51,484 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22028014063835144\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710fe7a8fb0e44d2b95b54e607e7cebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.192 MB of 8.192 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▂▄▄▄▃▃▃▂▂▃▅▂▂▃▂▂▂▃▁▃▂▃▁▃▂▂▃▂▂▂▂▁▃▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▄▄▃▃▃▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▅▅▃▃▃▃▂▂▃▂▂▂▂▂▁▂▁▁▂▂▂▂▁▁▃▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▅▅▃▃▃▃▃▂▃▂▃▂▂▂▂▂▁▁▂▂▂▂▁▁▃▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.22028</td></tr><tr><td>test_r2</td><td>0.82578</td></tr><tr><td>train_loss</td><td>0.02467</td></tr><tr><td>train_mae</td><td>0.20494</td></tr><tr><td>train_r2</td><td>0.84993</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.03928</td></tr><tr><td>val_mae</td><td>0.21749</td></tr><tr><td>val_r2</td><td>0.83001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-8</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/d195z0pb' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/d195z0pb</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_011233-d195z0pb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iiry5hyn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.001263670551615359\n",
      "2023-03-16 01:39:00,579 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 01:39:00,581 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_013901-iiry5hyn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/iiry5hyn' target=\"_blank\">jolly-sweep-9</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/iiry5hyn' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/iiry5hyn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 01:39:08,499 - INFO - Loading data and model\n",
      "2023-03-16 01:39:10,294 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 01:39:10,296 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 01:39:10,416 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 02:05:21,372 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22180086374282837\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebf13ef13574d15969cd8c8195e1e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.196 MB of 8.196 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▄▅▅▄▃▃▄▃▄▆▂▃▄▂▂▃▃▁▃▃▃▂▄▂▂▄▂▃▂▃▂▃▂▁▂▂▁▁</td></tr><tr><td>train_mae</td><td>█▅▄▄▃▄▃▃▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▅▃▃▂▃▂▂▃▂▂▂▂▁▁▁▁▁▂▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▄▅▃▃▃▃▂▂▃▂▂▂▂▁▁▁▁▁▂▁▂▂▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.2218</td></tr><tr><td>test_r2</td><td>0.82298</td></tr><tr><td>train_loss</td><td>0.02256</td></tr><tr><td>train_mae</td><td>0.19512</td></tr><tr><td>train_r2</td><td>0.86459</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.04029</td></tr><tr><td>val_mae</td><td>0.22094</td></tr><tr><td>val_r2</td><td>0.82557</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-sweep-9</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/iiry5hyn' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/iiry5hyn</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_013901-iiry5hyn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ma2oj392 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0008882273838737518\n",
      "2023-03-16 02:05:29,584 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 02:05:29,585 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_020530-ma2oj392</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/ma2oj392' target=\"_blank\">rich-sweep-10</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/ma2oj392' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/ma2oj392</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 02:05:37,337 - INFO - Loading data and model\n",
      "2023-03-16 02:05:39,066 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 02:05:39,068 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 02:05:39,184 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 02:31:44,621 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22066280245780945\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a37951c2f447fb805d5eafd9ced4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.209 MB of 8.209 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▇▄▄▃▂▂▂▂▃▄▂▂▃▂▁▂▃▁▃▂▂▁▃▂▃▃▂▂▁▂▁▃▁▁▂▁▁▁</td></tr><tr><td>train_mae</td><td>█▄▄▃▃▃▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▃▃▃▂▂▃▂▃▂▂▂▂▂▁▁▂▂▂▂▁▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▅▅▃▃▃▃▃▂▃▃▃▃▂▂▂▂▂▁▂▂▂▃▁▂▄▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.22066</td></tr><tr><td>test_r2</td><td>0.82533</td></tr><tr><td>train_loss</td><td>0.025</td></tr><tr><td>train_mae</td><td>0.20474</td></tr><tr><td>train_r2</td><td>0.85017</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.03942</td></tr><tr><td>val_mae</td><td>0.2175</td></tr><tr><td>val_r2</td><td>0.82942</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-sweep-10</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/ma2oj392' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/ma2oj392</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_020530-ma2oj392/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6p6zwzyk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 8.163058582453321e-05\n",
      "2023-03-16 02:31:53,244 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 02:31:53,245 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_023154-6p6zwzyk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/6p6zwzyk' target=\"_blank\">likely-sweep-11</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/6p6zwzyk' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/6p6zwzyk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 02:32:01,039 - INFO - Loading data and model\n",
      "2023-03-16 02:32:02,816 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 02:32:02,821 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 02:32:02,951 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 02:58:07,533 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.25414547324180603\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e951e56ffdf4b14990d6eda9686ed9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.221 MB of 8.221 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▇▆▆▄▃▆▄▄▄▃▃▄▄▄▂▄▃▂▄▄▃▃▄▃▂▄▃▃▃▃▁▄▂▃▃▃▂▃</td></tr><tr><td>train_mae</td><td>█▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▅▅▄▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▇▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▂▁▂▁▁▂▁▁▁▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.25415</td></tr><tr><td>test_r2</td><td>0.77176</td></tr><tr><td>train_loss</td><td>0.04146</td></tr><tr><td>train_mae</td><td>0.25425</td></tr><tr><td>train_r2</td><td>0.77054</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.05156</td></tr><tr><td>val_mae</td><td>0.25148</td></tr><tr><td>val_r2</td><td>0.77613</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-sweep-11</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/6p6zwzyk' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/6p6zwzyk</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_023154-6p6zwzyk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2o97tyff with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0021163457631158066\n",
      "2023-03-16 02:58:17,913 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 02:58:17,914 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_025818-2o97tyff</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/2o97tyff' target=\"_blank\">youthful-sweep-12</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/2o97tyff' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/2o97tyff</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 02:58:25,833 - INFO - Loading data and model\n",
      "2023-03-16 02:58:27,636 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 02:58:27,641 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 02:58:27,761 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 03:24:26,312 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22528231143951416\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7369d681d8094ffca1c2bf2761dd8d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.240 MB of 8.240 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>██▄▅▅▅▃▃▄▃▆▄▂▃▅▂▃▃▃▁▃▂▄▂▄▃▂▄▃▃▂▃▂▃▂▁▂▂▁▁</td></tr><tr><td>train_mae</td><td>█▅▄▄▃▄▃▃▂▂▄▃▃▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▅▅▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▂▁▁▂▁▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▅▆▂▃▂▂▂▁▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>1e-05</td></tr><tr><td>test_mae</td><td>0.22528</td></tr><tr><td>test_r2</td><td>0.8188</td></tr><tr><td>train_loss</td><td>0.02335</td></tr><tr><td>train_mae</td><td>0.19174</td></tr><tr><td>train_r2</td><td>0.86865</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.0418</td></tr><tr><td>val_mae</td><td>0.22484</td></tr><tr><td>val_r2</td><td>0.81883</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-sweep-12</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/2o97tyff' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/2o97tyff</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_025818-2o97tyff/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xl7xymys with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0004964490819497215\n",
      "2023-03-16 03:24:36,409 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 03:24:36,410 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_032437-xl7xymys</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/xl7xymys' target=\"_blank\">earthy-sweep-13</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/xl7xymys' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/xl7xymys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 03:24:44,203 - INFO - Loading data and model\n",
      "2023-03-16 03:24:46,097 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 03:24:46,099 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 03:24:46,211 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 03:50:47,479 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22335828840732574\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▅▄▅▃▂▃▃▂▃▅▂▂▄▃▂▃▃▁▃▃▃▂▃▃▃▄▂▂▂▃▁▃▁▁▂▂▂▂</td></tr><tr><td>train_mae</td><td>█▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▄▅▃▃▃▃▂▂▃▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▅▅▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.22336</td></tr><tr><td>test_r2</td><td>0.82186</td></tr><tr><td>train_loss</td><td>0.02846</td></tr><tr><td>train_mae</td><td>0.21786</td></tr><tr><td>train_r2</td><td>0.83038</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.04039</td></tr><tr><td>val_mae</td><td>0.22097</td></tr><tr><td>val_r2</td><td>0.82512</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-sweep-13</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/xl7xymys' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/xl7xymys</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_032437-xl7xymys/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8to1xxpb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0034596775777175686\n",
      "2023-03-16 03:51:00,687 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 03:51:00,688 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_035101-8to1xxpb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/8to1xxpb' target=\"_blank\">revived-sweep-14</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/8to1xxpb' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/8to1xxpb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 03:51:08,464 - INFO - Loading data and model\n",
      "2023-03-16 03:51:10,144 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 03:51:10,147 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 03:51:10,273 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 04:17:18,369 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.2650867998600006\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b488107cdd45109f08cf13c5379e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.272 MB of 8.272 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>▃▁▃▂▂▂▁▂▁▁▆▅▂▂▃▂▁▂▂▁█▆▄▃▅▃▃▃▃▃▃▃▂▃▂▂▂▃▂▂</td></tr><tr><td>train_mae</td><td>▄▂▂▂▁▂▁▁▁▁█▃▂▂▂▂▁▁▁▁█▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▅▂▂▃▁▁▁▂▁▁▄▃▃▂▂▂▂▂▁▁█▆▅▅▅▄▄▃▄▃▃▃▂▂▂▂▂▂▂▂</td></tr><tr><td>val_mae</td><td>▆▃▂▃▂▂▁▂▁▁▅▃▃▂▂▂▂▂▂▁█▆▅▅▅▄▄▄▄▃▃▄▃▃▂▃▂▂▂▂</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>1e-05</td></tr><tr><td>test_mae</td><td>0.26509</td></tr><tr><td>test_r2</td><td>0.74944</td></tr><tr><td>train_loss</td><td>0.04284</td></tr><tr><td>train_mae</td><td>0.26186</td></tr><tr><td>train_r2</td><td>0.75666</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.05652</td></tr><tr><td>val_mae</td><td>0.26291</td></tr><tr><td>val_r2</td><td>0.75471</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-sweep-14</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/8to1xxpb' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/8to1xxpb</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_035101-8to1xxpb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lk0305ue with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0010762859187571753\n",
      "2023-03-16 04:17:29,185 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-16 04:17:29,186 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_041730-lk0305ue</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/lk0305ue' target=\"_blank\">playful-sweep-15</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/nw1xnlx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/lk0305ue' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/lk0305ue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 04:17:37,044 - INFO - Loading data and model\n",
      "2023-03-16 04:17:38,828 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-16 04:17:38,832 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-16 04:17:38,950 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-16 04:43:50,260 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.21953141689300537\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae823f69a85f4824959cd95c4a3a14b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.292 MB of 8.292 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▇▅▅▄▃▂▃▃▅▄▂▂▄▂▂▂▃▁▃▂▃▁▄▂▃▃▂▂▂▃▁▃▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▅▄▄▃▃▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▂▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.21953</td></tr><tr><td>test_r2</td><td>0.82592</td></tr><tr><td>train_loss</td><td>0.02286</td></tr><tr><td>train_mae</td><td>0.19986</td></tr><tr><td>train_r2</td><td>0.85711</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.03989</td></tr><tr><td>val_mae</td><td>0.21925</td></tr><tr><td>val_r2</td><td>0.82725</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-sweep-15</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/lk0305ue' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/lk0305ue</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230316_041730-lk0305ue/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%wandb\n",
    "\n",
    "count = 15 # number of runs to execute\n",
    "wandb.agent(sweep_id, function=sweep_main, count=count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
