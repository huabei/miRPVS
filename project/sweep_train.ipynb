{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from main import main\n",
    "import ml_collections as mlc\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from model import MInterface\n",
    "from data import DInterface\n",
    "from train_utils import load_logger, load_callbacks\n",
    "from config.config import set_default_config\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "DEFAULT_CONFIG = mlc.ConfigDict()\n",
    "set_default_config(DEFAULT_CONFIG)\n",
    "with open('config/egnn_cfg_sweep.yaml', 'r') as f:\n",
    "    default_arg = yaml.safe_load(f)\n",
    "DEFAULT_CONFIG.update(default_arg)\n",
    "DEFAULT_CONFIG.log_dir = f'./log/sweep/{DEFAULT_CONFIG.pl_module.model_name}/{DEFAULT_CONFIG.pl_data_module.dataset}'\n",
    "if not os.path.exists(DEFAULT_CONFIG.log_dir):\n",
    "    os.makedirs(DEFAULT_CONFIG.log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sweep_main():\n",
    "    logging.info('seed everything')\n",
    "    pl.seed_everything(DEFAULT_CONFIG.seed)\n",
    "    if True:\n",
    "        logging.info('Using wandb')\n",
    "        wandb.init(project=DEFAULT_CONFIG.project, group=DEFAULT_CONFIG.group, save_code=True, dir=DEFAULT_CONFIG.log_dir, reinit=True)\n",
    "        args = DEFAULT_CONFIG\n",
    "        args.update_from_flattened_dict(vars(wandb.config)['_items'])\n",
    "        # raise Exception\n",
    "        wandb.config.update(args.pl_module.model.to_dict())\n",
    "        wandb.config.update(args.to_dict())\n",
    "    logging.info('Loading data and model')\n",
    "    data_module = DInterface(**args.pl_data_module)\n",
    "    model = MInterface(**args.pl_module)\n",
    "\n",
    "    logging.info('loading callbacks and logger')\n",
    "    args.trainer.callbacks = load_callbacks(args)\n",
    "    args.trainer.logger = load_logger(args)\n",
    "    \n",
    "    logging.info('creating trainer')\n",
    "    trainer = Trainer(**args.trainer)\n",
    "    \n",
    "    if args.trainer.auto_scale_batch_size:\n",
    "        logging.info('start auto scale batch size')\n",
    "        trainer.tune(model, data_module)\n",
    "\n",
    "    logging.info('start training')\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    logging.info('start testing')\n",
    "    trainer.test(model, data_module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 14:34:54,805 - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuabei\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for localhost to your netrc file: /home/huabei/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='local-8fe6e6b5840c4c05aaaf6aac5ca8c1fb58abbd1f', host='http://localhost:8080')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 08e5ghls\n",
      "Sweep URL: http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://docs.wandb.ai/guides/sweeps/define-sweep-configuration\n",
    "sweep_config = {\n",
    "  \"name\" : \"my-sweep\",\n",
    "  \"method\" : \"random\",\n",
    "  \"metric\" : {'goal': 'minimize', 'name': 'val_loss'},\n",
    "  \"parameters\" : {\n",
    "    \"pl_module.lr\": {\n",
    "      \"distribution\": \"log_uniform_values\",\n",
    "      \"min\": 0.00001,\n",
    "      \"max\": 0.001\n",
    "    },\n",
    "    # \"pl_module.model.tanh\": {\n",
    "    #   \"values\": [True, False]\n",
    "    # }\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=DEFAULT_CONFIG.project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 14:34:58,601 - INFO - Starting sweep agent: entity=None, project=None, count=10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vtc4ipuy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 2.374365506107818e-06\n",
      "2023-03-15 14:34:58,745 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 14:34:58,748 - INFO - Using wandb\n",
      "2023-03-15 14:34:58,752 - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_143459-vtc4ipuy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/vtc4ipuy' target=\"_blank\">lemon-sweep-1</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/vtc4ipuy' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/vtc4ipuy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 14:35:06,281 - INFO - Loading data and model\n",
      "2023-03-15 14:35:07,917 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "2023-03-15 14:35:07,947 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 14:35:08,086 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 15:00:40,895 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.3556353747844696\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e710fd5c65c44fd8b1bd7aa0b519668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.007 MB of 8.007 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.35564</td></tr><tr><td>test_r2</td><td>0.5584</td></tr><tr><td>train_loss</td><td>0.08919</td></tr><tr><td>train_mae</td><td>0.35797</td></tr><tr><td>train_r2</td><td>0.55613</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.09941</td></tr><tr><td>val_mae</td><td>0.3546</td></tr><tr><td>val_r2</td><td>0.56534</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-1</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/vtc4ipuy' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/vtc4ipuy</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_143459-vtc4ipuy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9epwwl63 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0007644679904092894\n",
      "2023-03-15 15:00:49,441 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 15:00:49,441 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_150050-9epwwl63</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/9epwwl63' target=\"_blank\">peachy-sweep-2</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/9epwwl63' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/9epwwl63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 15:00:56,913 - INFO - Loading data and model\n",
      "2023-03-15 15:00:58,738 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 15:00:58,739 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 15:00:58,853 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 15:26:53,672 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.2204955816268921\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f361a3ebc0eb4aadaf851d981a73cf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.011 MB of 8.011 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▅▄▅▃▃▃▃▃▃▄▂▃▄▃▂▃▃▁▃▂▃▁▃▂▂▃▂▂▂▃▁▄▁▁▂▂▂▂</td></tr><tr><td>train_mae</td><td>█▄▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▃▂▂▂▁▂▁▁▂▂▂▂▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▄▄▃▃▃▃▃▂▃▂▄▂▂▂▂▂▁▁▂▂▂▂▁▂▃▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.2205</td></tr><tr><td>test_r2</td><td>0.82564</td></tr><tr><td>train_loss</td><td>0.02753</td></tr><tr><td>train_mae</td><td>0.2088</td></tr><tr><td>train_r2</td><td>0.84409</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.03933</td></tr><tr><td>val_mae</td><td>0.21819</td></tr><tr><td>val_r2</td><td>0.82969</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-sweep-2</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/9epwwl63' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/9epwwl63</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_150050-9epwwl63/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rus1cpzn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 7.66810266062985e-06\n",
      "2023-03-15 15:27:02,625 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 15:27:02,626 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_152703-rus1cpzn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/rus1cpzn' target=\"_blank\">stellar-sweep-3</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/rus1cpzn' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/rus1cpzn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 15:27:09,955 - INFO - Loading data and model\n",
      "2023-03-15 15:27:11,760 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 15:27:11,762 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 15:27:11,871 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 15:52:45,597 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.3093579113483429\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d78d49d0bd4a6d9d290b1b802ae68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.021 MB of 8.021 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.30936</td></tr><tr><td>test_r2</td><td>0.66523</td></tr><tr><td>train_loss</td><td>0.06249</td></tr><tr><td>train_mae</td><td>0.31068</td></tr><tr><td>train_r2</td><td>0.66299</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.07521</td></tr><tr><td>val_mae</td><td>0.30692</td></tr><tr><td>val_r2</td><td>0.67246</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-3</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/rus1cpzn' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/rus1cpzn</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_152703-rus1cpzn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9t2qs9hr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.0005113576350965349\n",
      "2023-03-15 15:52:55,963 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 15:52:55,963 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_155256-9t2qs9hr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/9t2qs9hr' target=\"_blank\">expert-sweep-4</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/9t2qs9hr' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/9t2qs9hr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 15:53:03,025 - INFO - Loading data and model\n",
      "2023-03-15 15:53:04,837 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 15:53:04,839 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 15:53:04,936 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 16:18:56,133 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22231851518154144\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aab7a2fc5af424bb5869f975227a022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.026 MB of 8.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▅▄▅▃▂▃▃▃▃▅▂▂▄▂▂▃▃▁▃▂▃▂▃▂▃▄▂▂▂▃▁▃▁▁▂▂▂▂</td></tr><tr><td>train_mae</td><td>█▄▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▄▄▃▃▃▃▂▂▃▂▂▂▂▂▁▂▁▁▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▄▅▃▃▃▃▃▂▄▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.22232</td></tr><tr><td>test_r2</td><td>0.82277</td></tr><tr><td>train_loss</td><td>0.02748</td></tr><tr><td>train_mae</td><td>0.21599</td></tr><tr><td>train_r2</td><td>0.83341</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.0404</td></tr><tr><td>val_mae</td><td>0.22113</td></tr><tr><td>val_r2</td><td>0.82509</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">expert-sweep-4</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/9t2qs9hr' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/9t2qs9hr</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_155256-9t2qs9hr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yutsquqs with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 1.353741946668555e-05\n",
      "2023-03-15 16:19:04,958 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 16:19:04,959 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_161905-yutsquqs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/yutsquqs' target=\"_blank\">soft-sweep-5</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/yutsquqs' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/yutsquqs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 16:19:12,298 - INFO - Loading data and model\n",
      "2023-03-15 16:19:14,147 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 16:19:14,151 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 16:19:14,276 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 16:45:05,205 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.2883140742778778\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdbf1b6abbb454d9197c9e8fd073e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.035 MB of 8.035 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▄▃▃▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▂▁▂▂▁▂▂▂▂▂▁▂▁▂▁▂▁▁</td></tr><tr><td>train_mae</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▄▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▆▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.28831</td></tr><tr><td>test_r2</td><td>0.70745</td></tr><tr><td>train_loss</td><td>0.05418</td></tr><tr><td>train_mae</td><td>0.28949</td></tr><tr><td>train_r2</td><td>0.70542</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.06573</td></tr><tr><td>val_mae</td><td>0.28586</td></tr><tr><td>val_r2</td><td>0.7142</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-sweep-5</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/yutsquqs' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/yutsquqs</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_161905-yutsquqs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jigahqod with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 6.3849204104305864e-06\n",
      "2023-03-15 16:45:13,845 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 16:45:13,847 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_164514-jigahqod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/jigahqod' target=\"_blank\">icy-sweep-6</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/jigahqod' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/jigahqod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 16:45:21,275 - INFO - Loading data and model\n",
      "2023-03-15 16:45:23,249 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 16:45:23,253 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 16:45:23,371 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 17:11:13,247 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.3174159526824951\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb24f97a257c4d56aa5e73ccd9e92719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.044 MB of 8.044 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.31742</td></tr><tr><td>test_r2</td><td>0.64788</td></tr><tr><td>train_loss</td><td>0.06688</td></tr><tr><td>train_mae</td><td>0.31879</td></tr><tr><td>train_r2</td><td>0.64571</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.07921</td></tr><tr><td>val_mae</td><td>0.31539</td></tr><tr><td>val_r2</td><td>0.65477</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-sweep-6</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/jigahqod' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/jigahqod</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_164514-jigahqod/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fmgpr9ez with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 2.554529586164634e-05\n",
      "2023-03-15 17:11:22,695 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 17:11:22,696 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_171123-fmgpr9ez</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/fmgpr9ez' target=\"_blank\">chocolate-sweep-7</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/fmgpr9ez' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/fmgpr9ez</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 17:11:29,985 - INFO - Loading data and model\n",
      "2023-03-15 17:11:31,864 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 17:11:31,866 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 17:11:31,987 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 17:36:58,953 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.26845356822013855\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96c1635f9a64138a15cd91cef51ba12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.053 MB of 8.053 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▆▅▅▄▃▅▄▄▃▄▃▃▄▃▂▄▂▃▃▃▃▃▃▂▂▃▂▂▂▂▁▃▂▂▂▂▂▂</td></tr><tr><td>train_mae</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▆▅▅▅▄▄▄▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.26845</td></tr><tr><td>test_r2</td><td>0.7448</td></tr><tr><td>train_loss</td><td>0.04669</td></tr><tr><td>train_mae</td><td>0.26876</td></tr><tr><td>train_r2</td><td>0.74436</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.05751</td></tr><tr><td>val_mae</td><td>0.26611</td></tr><tr><td>val_r2</td><td>0.75023</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-sweep-7</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/fmgpr9ez' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/fmgpr9ez</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_171123-fmgpr9ez/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: piphqxba with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 1.1093810130544112e-06\n",
      "2023-03-15 17:37:07,833 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 17:37:07,833 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_173708-piphqxba</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/piphqxba' target=\"_blank\">deep-sweep-8</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/piphqxba' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/piphqxba</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 17:37:15,192 - INFO - Loading data and model\n",
      "2023-03-15 17:37:16,993 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 17:37:16,995 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 17:37:17,109 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 18:02:46,077 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.3763151466846466\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8d9c76d03446b6a54345efb5febb66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.061 MB of 8.061 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▆▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.37632</td></tr><tr><td>test_r2</td><td>0.50773</td></tr><tr><td>train_loss</td><td>0.09971</td></tr><tr><td>train_mae</td><td>0.37856</td></tr><tr><td>train_r2</td><td>0.50429</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.11023</td></tr><tr><td>val_mae</td><td>0.37392</td></tr><tr><td>val_r2</td><td>0.51691</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-sweep-8</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/piphqxba' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/piphqxba</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_173708-piphqxba/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kxblayag with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 1.5610944677887784e-06\n",
      "2023-03-15 18:02:56,068 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 18:02:56,069 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_180256-kxblayag</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/kxblayag' target=\"_blank\">legendary-sweep-9</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/kxblayag' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/kxblayag</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 18:03:03,552 - INFO - Loading data and model\n",
      "2023-03-15 18:03:05,432 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 18:03:05,435 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 18:03:05,565 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 18:28:40,010 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.36788666248321533\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470d692a3dd44c27a7473860c6681b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.066 MB of 8.066 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.36789</td></tr><tr><td>test_r2</td><td>0.52842</td></tr><tr><td>train_loss</td><td>0.09517</td></tr><tr><td>train_mae</td><td>0.3703</td></tr><tr><td>train_r2</td><td>0.52548</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.1058</td></tr><tr><td>val_mae</td><td>0.36615</td></tr><tr><td>val_r2</td><td>0.53677</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-sweep-9</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/kxblayag' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/kxblayag</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_180256-kxblayag/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s6c7bizi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpl_module.lr: 0.00024657458195004\n",
      "2023-03-15 18:28:49,144 - INFO - seed everything\n",
      "Global seed set to 1234\n",
      "2023-03-15 18:28:49,145 - INFO - Using wandb\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "warning: project/sweep_train.ipynb 中的 LF 将被 CRLF 替换。\n",
      "在工作区中该文件仍保持原有的换行符\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_182850-s6c7bizi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/huabei/egnn-sweep/runs/s6c7bizi' target=\"_blank\">different-sweep-10</a></strong> to <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/huabei/egnn-sweep' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/sweeps/08e5ghls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/huabei/egnn-sweep/runs/s6c7bizi' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/s6c7bizi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 18:28:56,467 - INFO - Loading data and model\n",
      "2023-03-15 18:28:58,262 - INFO - loading callbacks and logger\n",
      "/home/huabei/miniconda3/envs/pytorch112/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning:\n",
      "\n",
      "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "2023-03-15 18:28:58,264 - INFO - creating trainer\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-03-15 18:28:58,369 - INFO - start training\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=63` reached.\n",
      "2023-03-15 18:54:44,701 - INFO - start testing\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_mae            0.22933407127857208\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c03de65ba94c15b27f303b9ee79737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.073 MB of 8.073 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁</td></tr><tr><td>lr-Adam</td><td>███▇▂█▇▄▃▁██▇▆▅▄▃▂▂▁████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▄▅▆▄▃▄▃▃▄▅▃▃▅▃▂▃▃▂▄▃▃▁▄▃▃▄▂▃▂▃▁▃▂▁▂▂▂▂</td></tr><tr><td>train_mae</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▆▅▄▃▃▃▄▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_r2</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>global_step</td><td>39564</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>test_mae</td><td>0.22933</td></tr><tr><td>test_r2</td><td>0.81307</td></tr><tr><td>train_loss</td><td>0.03222</td></tr><tr><td>train_mae</td><td>0.22735</td></tr><tr><td>train_r2</td><td>0.81582</td></tr><tr><td>trainer/global_step</td><td>39564</td></tr><tr><td>val_loss</td><td>0.04228</td></tr><tr><td>val_mae</td><td>0.22668</td></tr><tr><td>val_r2</td><td>0.81686</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-sweep-10</strong> at: <a href='http://localhost:8080/huabei/egnn-sweep/runs/s6c7bizi' target=\"_blank\">http://localhost:8080/huabei/egnn-sweep/runs/s6c7bizi</a><br/>Synced 7 W&B file(s), 3 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./log/sweep/egnn/zinc_complex3a6p_data/wandb/run-20230315_182850-s6c7bizi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%wandb\n",
    "\n",
    "count = 10 # number of runs to execute\n",
    "wandb.agent(sweep_id, function=sweep_main, count=count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
